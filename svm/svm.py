# First, Decide on your approach for evaluating your results. What accuracy measures are you
# going to use? Should all mistakes be treated the same? Next, prepare for your experiments.
# For the datasets without a test set, you will want to create a hold-out testing set that you don’t
# peek at until the very end! Then, explore your data, decide on your feature representation,
# sample appropriately (for the larger datasets you might start with a smaller subset), and set up
# your training data. Note that feature representation might be harder or easier for some
# datasets, and evaluation might take more / less time for some of the datasets. Next, explore
# the baseline accuracy of your chosen algorithms, and decide whether hyper-parameter tuning
# might be appropriate. You’ll want to evaluate the cross-validation performance of your chosen
# evaluation metric. Finally, for the “best” model from each algorithm, apply them to the test set
# and again apply your chosen metric. Which models win? You won’t get graded as much on the
# accuracy of your results as the care and insights applied during the evaluation process (but
# see also the extra credit opportunity below).

